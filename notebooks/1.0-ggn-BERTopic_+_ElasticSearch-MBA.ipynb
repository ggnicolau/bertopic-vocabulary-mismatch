{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyYFDDtEaaiO"
   },
   "source": [
    "# Instalando ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOKEpb4uaK7-",
    "outputId": "6b638c77-acbb-40cf-de94-5704cd3f3811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "useradd: user 'elasticsearch' already exists\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 269, in run\n",
      "    session = self.get_default_session(options)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 77, in get_default_session\n",
      "    self._session = self.enter_context(self._build_session(options))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 93, in _build_session\n",
      "    index_urls=self._get_index_urls(options),\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/network/session.py\", line 275, in __init__\n",
      "    self.headers[\"User-Agent\"] = user_agent()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/network/session.py\", line 129, in user_agent\n",
      "    from pip._vendor import distro\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/distro.py\", line 83, in <module>\n",
      "    r'(?:[^)]*\\)(.*)\\()? *(?:STL )?([\\d.+\\-a-z]*\\d) *(?:esaeler *)?(.+)')\n",
      "  File \"/usr/lib/python3.7/re.py\", line 236, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"/usr/lib/python3.7/re.py\", line 288, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"/usr/lib/python3.7/sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"/usr/lib/python3.7/sre_parse.py\", line 924, in parse\n",
      "    p = _parse_sub(source, pattern, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"/usr/lib/python3.7/sre_parse.py\", line 420, in _parse_sub\n",
      "    not nested and not items))\n",
      "  File \"/usr/lib/python3.7/sre_parse.py\", line 810, in _parse\n",
      "    p = _parse_sub(source, state, sub_verbose, nested + 1)\n",
      "  File \"/usr/lib/python3.7/sre_parse.py\", line 420, in _parse_sub\n",
      "    not nested and not items))\n",
      "  File \"/usr/lib/python3.7/sre_parse.py\", line 810, in _parse\n",
      "    p = _parse_sub(source, state, sub_verbose, nested + 1)\n",
      "  File \"/usr/lib/python3.7/sre_parse.py\", line 420, in _parse_sub\n",
      "    not nested and not items))\n",
      "  File \"/usr/lib/python3.7/sre_parse.py\", line 469, in _parse\n",
      "    def _parse(source, state, verbose, nested, first=False):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 71, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 104, in main\n",
      "    return self._main(args)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 213, in _main\n",
      "    logger.debug(\"Exception information:\", exc_info=True)\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1366, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1514, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1524, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1586, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 894, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.7/logging/handlers.py\", line 71, in emit\n",
      "    logging.FileHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1127, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1025, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 869, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/logging.py\", line 130, in format\n",
      "    formatted = super().format(record)\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 616, in format\n",
      "    record.exc_text = self.formatException(record.exc_info)\n",
      "  File \"/usr/lib/python3.7/logging/__init__.py\", line 566, in formatException\n",
      "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
      "  File \"/usr/lib/python3.7/traceback.py\", line 104, in print_exception\n",
      "    type(value), value, tb, limit=limit).format(chain=chain):\n",
      "  File \"/usr/lib/python3.7/traceback.py\", line 508, in __init__\n",
      "    capture_locals=capture_locals)\n",
      "  File \"/usr/lib/python3.7/traceback.py\", line 363, in extract\n",
      "    f.line\n",
      "  File \"/usr/lib/python3.7/traceback.py\", line 285, in line\n",
      "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "  File \"/usr/lib/python3.7/linecache.py\", line 16, in getline\n",
      "    lines = getlines(filename, module_globals)\n",
      "  File \"/usr/lib/python3.7/linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/usr/lib/python3.7/linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"/usr/lib/python3.7/tokenize.py\", line 449, in open\n",
      "    encoding, lines = detect_encoding(buffer.readline)\n",
      "  File \"/usr/lib/python3.7/tokenize.py\", line 418, in detect_encoding\n",
      "    first = read_or_stop()\n",
      "  File \"/usr/lib/python3.7/tokenize.py\", line 376, in read_or_stop\n",
      "    return readline()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.10.0-linux-x86_64.tar.gz -q\n",
    "!tar -xzf elasticsearch-7.10.0-linux-x86_64.tar.gz\n",
    "!chown -R daemon:daemon elasticsearch-7.10.0-linux-x86_64.tar.gz\n",
    "!useradd elasticsearch\n",
    "!chmod -R 777 elasticsearch-7.10.0\n",
    "!echo \"!#/bin/bash\" > /content/run.sh\n",
    "!echo \"/content/elasticsearch-7.10.0/bin/elasticsearch &\" >> /content/run.sh\n",
    "!chmod 777 run.sh\n",
    "# !export JAVA_HOME=\"/content/elasticsearch-7.10.0/jdk\"; sudo -E -u elasticsearch nohup /content/run.sh  > /content/elasticsearch.run.log &\n",
    "!pip install elasticsearch==7.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jKMpGWa7y7vt",
    "outputId": "a9dcf9e5-30e4-499f-c74d-a52dbb5fa0e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: redirecting stderr to stdout\n"
     ]
    }
   ],
   "source": [
    "!export JAVA_HOME=\"/content/elasticsearch-7.10.0/jdk\"; sudo -E -u elasticsearch nohup /content/run.sh  > /content/elasticsearch.run.log &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubOnSu3vpqH6"
   },
   "source": [
    "# Indexando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuNwQvWLB4qX"
   },
   "outputs": [],
   "source": [
    "index_data = '''{\n",
    "  \"settings\": {\n",
    "    \"number_of_shards\": 2,\n",
    "    \"number_of_replicas\": 1\n",
    "  },\n",
    "   \"mappings\": {\n",
    "    \"dynamic\": \"true\",\n",
    "    \"_source\": {\n",
    "      \"enabled\": \"true\"\n",
    "    },\n",
    "    \"properties\": {\n",
    "      \"text\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"topic\": {\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}'''\n",
    "\n",
    "f = open(\"index.json\", \"w\")\n",
    "f.write(index_data)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1V84wrHxHmf2"
   },
   "outputs": [],
   "source": [
    "# esperar um tempo até o elasticsearch subir\n",
    "!sleep 40 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JDT1iI0pvmN"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "client = Elasticsearch()\n",
    "\n",
    "INDEX_NAME = 'teste'\n",
    "\n",
    "client.indices.delete(index=INDEX_NAME, ignore=[404])\n",
    "\n",
    "with open('/content/index.json') as index_file:\n",
    "  source = index_file.read().strip()\n",
    "  client.indices.create(index=INDEX_NAME, body=source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OowmhYlC0-5"
   },
   "source": [
    "# Etapa 1: Inserindo Dados Textuais para Ranking BM25\n",
    "\n",
    "https://en.wikipedia.org/wiki/Okapi_BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cXsElwL7C3CV",
    "outputId": "33ae70a9-4151-4428-a1bb-18bdc2c5a0cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-07 20:19:18--  https://raw.githubusercontent.com/rmarcacini/text-collections/master/complete_texts_csvs/SyskillWebert.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 783193 (765K) [text/plain]\n",
      "Saving to: ‘SyskillWebert.csv’\n",
      "\n",
      "SyskillWebert.csv   100%[===================>] 764.84K  --.-KB/s    in 0.008s  \n",
      "\n",
      "2022-07-07 20:19:18 (95.4 MB/s) - ‘SyskillWebert.csv’ saved [783193/783193]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/rmarcacini/text-collections/master/complete_texts_csvs/classic4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "FfYZLeR_DZWi",
    "outputId": "30e55d16-9013-40a9-be6a-0b922b52e18e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-6fd2fe35-5c1a-45bd-90f1-31f9f932202f\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>LaFond, Lois        One World              (...</td>\n",
       "      <td>Bands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>Houston, Penelope   Houston, Penelope       ...</td>\n",
       "      <td>Bands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>Bernard, Mary Ellen   Bernard, Mary Ellen   ...</td>\n",
       "      <td>Bands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>Deth Specula   Deth Specula         Careenin...</td>\n",
       "      <td>Bands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>Hungry Ghost   Hungry Ghost         Aboral  ...</td>\n",
       "      <td>Bands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>60</td>\n",
       "      <td>Too Busy Too Busy!</td>\n",
       "      <td>Sheep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>52</td>\n",
       "      <td>Desert Bighorn Sheep       Desert Bighorn ...</td>\n",
       "      <td>Sheep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>39</td>\n",
       "      <td>Wool Manufacturing: Shear Pain    Miscella...</td>\n",
       "      <td>Sheep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>19</td>\n",
       "      <td>Sheep Brain Atlas   Saggital View  Click on ...</td>\n",
       "      <td>Sheep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>20</td>\n",
       "      <td>LCTGM--Mountain sheep                Mounta...</td>\n",
       "      <td>Sheep</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>334 rows × 3 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6fd2fe35-5c1a-45bd-90f1-31f9f932202f')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-6fd2fe35-5c1a-45bd-90f1-31f9f932202f button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-6fd2fe35-5c1a-45bd-90f1-31f9f932202f');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "    file_name                                               text  class\n",
       "0          53    LaFond, Lois        One World              (...  Bands\n",
       "1          28    Houston, Penelope   Houston, Penelope       ...  Bands\n",
       "2          57    Bernard, Mary Ellen   Bernard, Mary Ellen   ...  Bands\n",
       "3          25    Deth Specula   Deth Specula         Careenin...  Bands\n",
       "4          15    Hungry Ghost   Hungry Ghost         Aboral  ...  Bands\n",
       "..        ...                                                ...    ...\n",
       "329        60                                Too Busy Too Busy!   Sheep\n",
       "330        52      Desert Bighorn Sheep       Desert Bighorn ...  Sheep\n",
       "331        39      Wool Manufacturing: Shear Pain    Miscella...  Sheep\n",
       "332        19    Sheep Brain Atlas   Saggital View  Click on ...  Sheep\n",
       "333        20     LCTGM--Mountain sheep                Mounta...  Sheep\n",
       "\n",
       "[334 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_docs = pd.read_csv('classic4.csv')\n",
    "df_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "LV8jcqWaoGf1",
    "outputId": "e57f2f8f-fb5e-4f84-a55a-d51cfa63d6e0"
   },
   "outputs": [
    {
     "ename": "BulkIndexError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBulkIndexError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7618558b596d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mbulk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/elasticsearch/helpers/actions.py\u001b[0m in \u001b[0;36mbulk\u001b[0;34m(client, actions, stats_only, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"yield_ok\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     for ok, item in streaming_bulk(\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_status\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     ):\n\u001b[1;32m    413\u001b[0m         \u001b[0;31m# go through request-response pairs and detect failures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/elasticsearch/helpers/actions.py\u001b[0m in \u001b[0;36mstreaming_bulk\u001b[0;34m(client, actions, chunk_size, max_chunk_bytes, raise_on_error, expand_action_callback, raise_on_exception, max_retries, initial_backoff, max_backoff, yield_ok, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m                         \u001b[0mignore_status\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                     ),\n\u001b[1;32m    341\u001b[0m                 ):\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/elasticsearch/helpers/actions.py\u001b[0m in \u001b[0;36m_process_bulk_chunk\u001b[0;34m(client, bulk_actions, bulk_data, raise_on_exception, raise_on_error, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mraise_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         )\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/elasticsearch/helpers/actions.py\u001b[0m in \u001b[0;36m_process_bulk_chunk_success\u001b[0;34m(resp, bulk_data, ignore_status, raise_on_error)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mBulkIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%i document(s) failed to index.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBulkIndexError\u001b[0m: ('1 document(s) failed to index.', [{'index': {'_index': 'teste', '_type': '_doc', '_id': '1nBR2oEBX-9hPJ6_oTeU', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': 'failed to parse', 'caused_by': {'type': 'json_parse_exception', 'reason': 'Non-standard token \\'NaN\\': enable JsonParser.Feature.ALLOW_NON_NUMERIC_NUMBERS to allow\\n at [Source: (byte[])\"d Swine Continuing Education Offerings: Sheep, Goats and Swine      Service   Liaison for farm advisors; farm bureau; sheep, goat and swine producers California Wool Growers Association Animal Health and Ram Sale Committees; Chair, USDA National Oversight Committee for Voluntary Scrapie Flock Certification Program      International Interest   Australia, Bolivia, Caribbean, Indonesia, Morocco, New Zealand      Foreign Languages         \",\"class\":\"Goats\"}\\n{\"index\":{\"_index\":\"teste\"}}\\n{\"file_name\"\"[truncated 65036 bytes]; line: 1, column: 29]'}}, 'data': {'file_name': '11', 'text': nan, 'class': 'Goats'}}}])"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "#### conectando ao elastic search\n",
    "client = Elasticsearch()\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import urllib.request, json \n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "counter = 0\n",
    "\n",
    "requests = []\n",
    "for index,row in df_docs.iterrows():\n",
    "  request = row.to_dict()\n",
    "  request[\"_op_type\"] = \"index\"\n",
    "  request[\"_index\"] = INDEX_NAME\n",
    "  requests.append(request)\n",
    "\n",
    "  if len(requests) >= 10000:\n",
    "    bulk(client, requests)\n",
    "    request = []\n",
    "\n",
    "bulk(client, requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnmSNGyGDwsJ"
   },
   "source": [
    "### Testando uma query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38y0ymos3s89"
   },
   "outputs": [],
   "source": [
    "import urllib.request, json \n",
    "\n",
    "query = urllib.parse.quote_plus('neural networks for linux systems')\n",
    "\n",
    "L = []\n",
    "with urllib.request.urlopen(\"http://localhost:9200/_search/?q=\"+query+\"&size=50\") as url:\n",
    "    data = json.loads(url.read().decode())\n",
    "    for item in data['hits']['hits']:\n",
    "      try:\n",
    "        L.append(item['_source'])\n",
    "      except:\n",
    "        1\n",
    "    #print(data)\n",
    "\n",
    "df_results = pd.DataFrame(L)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyXBbCh4LhoS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import e\n",
    "import pandas as pd   \n",
    "\n",
    "def df_entropy(column, base=None):\n",
    "    vc = pd.Series(column).value_counts(normalize=True, sort=False)\n",
    "    #print(vc)\n",
    "    base = len(vc.index)\n",
    "    entropy = -(vc * np.log(vc)/np.log(base)).sum()\n",
    "    #print('Entropy: ',entropy)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_l5ktyF7LlT0"
   },
   "outputs": [],
   "source": [
    "df_entropy(df_results['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZ2NHjzHM0og"
   },
   "source": [
    "## Avaliando Múltiplas Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWpwWGIWNDEl"
   },
   "source": [
    "Queries candidatas por meio de ngramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGUUoKbsNFZ2"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZ7RMYoYNrwR"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNxxKvpuNnsM"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text,stop_words):\n",
    "  \n",
    "  # tudo para caixa baixa\n",
    "  s = str(text).lower() \n",
    "\n",
    "  tokens = word_tokenize(s)\n",
    "\n",
    "  # remove stopwords, dígitos, caracteres especiais e pontuações\n",
    "  v = [word for word in tokens if not word in stop_words and word.isalnum() and not word.isdigit()]\n",
    "\n",
    "  return v\n",
    "\n",
    "def meu_tokenizador(doc, stop_words=nltk.corpus.stopwords.words('english'), stemmer=PorterStemmer()):\n",
    "  tokens = remove_stopwords(doc,stop_words)\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHYYeqLSNcM4"
   },
   "outputs": [],
   "source": [
    "VSM = TfidfVectorizer(tokenizer=meu_tokenizador,min_df=3,ngram_range=(2,3))\n",
    "X = VSM.fit_transform(df_docs['text'])\n",
    "\n",
    "df_ngrams = pd.DataFrame()\n",
    "df_ngrams['word'] = VSM.get_feature_names()\n",
    "df_ngrams['tfidf_sum'] = X.toarray().sum(axis=0)\n",
    "df_ngrams.sort_values(by='tfidf_sum',ascending=False,inplace=True)\n",
    "df_ngrams.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7sPdGr4QPO6"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "R = []\n",
    "for index,row in tqdm(df_ngrams.head(1000).iterrows(),total=len(df_ngrams)):\n",
    "\n",
    "\n",
    "  query = urllib.parse.quote_plus(row['word'])\n",
    "\n",
    "  L = []\n",
    "  with urllib.request.urlopen(\"http://localhost:9200/_search/?q=\"+query+\"&size=50\") as url:\n",
    "      data = json.loads(url.read().decode())\n",
    "      for item in data['hits']['hits']:\n",
    "        try:\n",
    "          L.append(item['_source'])\n",
    "        except:\n",
    "          1\n",
    "      #print(data)\n",
    "  \n",
    "  df_results = pd.DataFrame(L)\n",
    "  entropy = df_entropy(df_results['class'])\n",
    "  R.append([query,entropy,len(df_results),df_results['class'].to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kI9-B7pUQPO8"
   },
   "outputs": [],
   "source": [
    "df_results_baseline = pd.DataFrame(R)\n",
    "df_results_baseline.columns =['query','entropy','size','labels']\n",
    "df_results_baseline.to_parquet('df_results_baseline_classic4.parquet.gzip',compression='gzip')\n",
    "df_results_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71XWO0MRQPO9"
   },
   "outputs": [],
   "source": [
    "df_results_baseline.entropy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk5ULQrYFdAz"
   },
   "source": [
    "# Etapa 2: Enriquecendo documentos com informação de tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t729ne_AF3RP"
   },
   "outputs": [],
   "source": [
    "!pip install bertopic\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MTijycaD7eJ"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "stopwords_en = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words=stopwords_en)\n",
    "\n",
    "topic_model = BERTopic(embedding_model=\"all-mpnet-base-v2\",\n",
    "                       nr_topics=300,\n",
    "                       vectorizer_model=vectorizer_model)\n",
    "\n",
    "\n",
    "topics = topic_model.fit_transform(df_docs.text.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tl2dMuBzJFjd"
   },
   "outputs": [],
   "source": [
    "L_topics = []\n",
    "for t in topics[0]:\n",
    "  if t!=-1:\n",
    "    topic_data = topic_model.get_topic(t)\n",
    "    topic_words = ''\n",
    "    for v in topic_data:\n",
    "      topic_words += v[0]+' '\n",
    "    L_topics.append(topic_words)\n",
    "  else: L_topics.append('')\n",
    "\n",
    "df_docs['topic'] = L_topics\n",
    "\n",
    "df_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGJgUnkCGSuG"
   },
   "source": [
    "## Reindexando (com informação de tópico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qpqtVk-GV6E"
   },
   "outputs": [],
   "source": [
    "client.indices.delete(index=INDEX_NAME, ignore=[404])\n",
    "\n",
    "with open('/content/index.json') as index_file:\n",
    "  source = index_file.read().strip()\n",
    "  client.indices.create(index=INDEX_NAME, body=source)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "requests = []\n",
    "for index,row in df_docs.iterrows():\n",
    "  request = row.to_dict()\n",
    "  request[\"_op_type\"] = \"index\"\n",
    "  request[\"_index\"] = INDEX_NAME\n",
    "  requests.append(request)\n",
    "\n",
    "  if len(requests) >= 10000:\n",
    "    bulk(client, requests)\n",
    "    request = []\n",
    "\n",
    "bulk(client, requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0bp_uzZKQAJ"
   },
   "source": [
    "### Testando query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwB72GEXKRMJ"
   },
   "outputs": [],
   "source": [
    "query = urllib.parse.quote_plus('neural networks for linux systems')\n",
    "\n",
    "L = []\n",
    "with urllib.request.urlopen(\"http://localhost:9200/_search/?q=\"+query+\"&size=50\") as url:\n",
    "    data = json.loads(url.read().decode())\n",
    "    for item in data['hits']['hits']:\n",
    "      try:\n",
    "        L.append(item['_source'])\n",
    "      except:\n",
    "        1\n",
    "    #print(data)\n",
    "\n",
    "df_results = pd.DataFrame(L)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOs_i0hoLBKj"
   },
   "outputs": [],
   "source": [
    "df_entropy(df_results['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHiW3_zUOgQx"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "R = []\n",
    "for index,row in tqdm(df_ngrams.head(1000).iterrows(),total=len(df_ngrams)):\n",
    "\n",
    "\n",
    "  query = urllib.parse.quote_plus(row['word'])\n",
    "\n",
    "  L = []\n",
    "  with urllib.request.urlopen(\"http://localhost:9200/_search/?q=\"+query+\"&size=50\") as url:\n",
    "      data = json.loads(url.read().decode())\n",
    "      for item in data['hits']['hits']:\n",
    "        try:\n",
    "          L.append(item['_source'])\n",
    "        except:\n",
    "          1\n",
    "      #print(data)\n",
    "  \n",
    "  df_results = pd.DataFrame(L)\n",
    "  entropy = df_entropy(df_results['class'])\n",
    "  R.append([query,entropy,len(df_results),df_results['class'].to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lgP35RnPVai"
   },
   "outputs": [],
   "source": [
    "df_results_proposal = pd.DataFrame(R)\n",
    "df_results_proposal.columns =['query','entropy','size','labels']\n",
    "df_results_proposal.entropy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dY67loOXPWsq"
   },
   "outputs": [],
   "source": [
    "df_results_proposal.to_parquet('df_results_proposal_classic4.parquet.gzip',compression='gzip')\n",
    "df_results_proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYf-n8l-Ps7r"
   },
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRPPDmFOLvYT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Cópia de BERTopic + ElasticSearch - MBA - Nicolau - VERSAO PARA EXPERIMENTOS.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
